{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Aula 2 - CDBD 8_2.ipynb","provenance":[],"collapsed_sections":["Nzfi_ddLMdSL","Ir7FeO9UMdTt","qATHKSu5MdVD","40udJCqFMdVO"]}},"cells":[{"cell_type":"code","metadata":{"id":"mtj0s4WK4pHB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600997476463,"user_tz":180,"elapsed":1977,"user":{"displayName":"Bárbara Silveira Fraga","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRSYKhKST3-JxDhaPKtydW_1gnhsEFC_X_IjaV=s64","userId":"08155615283259294201"}}},"source":["import nltk"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdsKwZQC4r6x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"257261a4-91fc-467b-ba1b-b6da1e5ae909"},"source":["nltk.download()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> d\n","\n","Download which package (l=list; x=cancel)?\n","  Identifier> l\n","Packages:\n","  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n","  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n","  [ ] basque_grammars..... Grammars for Basque\n","  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n","  [ ] book_grammars....... Grammars from NLTK Book\n","  [ ] inaugural........... C-Span Inaugural Address Corpus\n","  [ ] indian.............. Indian Language POS-Tagged Corpus\n","  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n","                           ChaSen format)\n","  [ ] kimmo............... PC-KIMMO Data Files\n","  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n","  [ ] large_grammars...... Large context-free and feature-based grammars\n","                           for parser comparison\n","  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n","  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n","                           part-of-speech tags\n","  [ ] machado............. Machado de Assis -- Obra Completa\n","  [ ] masc_tagged......... MASC Tagged Corpus\n","  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n","  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n","Hit Enter to continue: \n","  [ ] moses_sample........ Moses Sample Models\n","  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n","  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n","  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n","                           2015) subset of the Paraphrase Database.\n","  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n","  [ ] nombank.1.0......... NomBank Corpus 1.0\n","  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n","  [ ] nps_chat............ NPS Chat\n","  [ ] omw................. Open Multilingual Wordnet\n","  [ ] opinion_lexicon..... Opinion Lexicon\n","  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n","  [ ] paradigms........... Paradigm Corpus\n","  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n","                           Evaluation Shared Task\n","  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n","                           character properties in Perl\n","  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n","  [ ] pl196x.............. Polish language of the XX century sixties\n","  [ ] porter_test......... Porter Stemmer Test Files\n","  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n","Hit Enter to continue: \n","Hit Enter to continue: \n","  [ ] problem_reports..... Problem Report Corpus\n","  [ ] product_reviews_1... Product Reviews (5 Products)\n","  [ ] product_reviews_2... Product Reviews (9 Products)\n","  [ ] propbank............ Proposition Bank Corpus 1.0\n","  [ ] pros_cons........... Pros and Cons\n","  [ ] ptb................. Penn Treebank\n","  [ ] punkt............... Punkt Tokenizer Models\n","  [ ] qc.................. Experimental Data for Question Classification\n","  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n","                           version\n","  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n","                           Portuguesa)\n","  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n","  [ ] sample_grammars..... Sample Grammars\n","  [ ] semcor.............. SemCor 3.0\n","  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n","  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n","  [ ] sentiwordnet........ SentiWordNet\n","  [ ] shakespeare......... Shakespeare XML Corpus Sample\n","  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n","  [ ] smultron............ SMULTRON Corpus Sample\n","  [ ] snowball_data....... Snowball Data\n","  [ ] spanish_grammars.... Grammars for Spanish\n","  [ ] state_union......... C-Span State of the Union Address Corpus\n","  [ ] stopwords........... Stopwords Corpus\n","  [ ] subjectivity........ Subjectivity Dataset v1.0\n","  [ ] swadesh............. Swadesh Wordlists\n","  [ ] switchboard......... Switchboard Corpus Sample\n","  [ ] tagsets............. Help on Tagsets\n","  [ ] timit............... TIMIT Corpus Sample\n","  [ ] toolbox............. Toolbox Sample Files\n","  [ ] treebank............ Penn Treebank Sample\n","  [ ] twitter_samples..... Twitter Samples\n","  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n","                           (Unicode Version)\n","  [ ] udhr................ Universal Declaration of Human Rights Corpus\n","  [ ] unicode_samples..... Unicode Samples\n","  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n","  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n","  [ ] vader_lexicon....... VADER Sentiment Lexicon\n","  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n","  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n","Hit Enter to continue: \n","  [ ] webtext............. Web Text Corpus\n","  [ ] wmt15_eval.......... Evaluation data from WMT15\n","  [ ] word2vec_sample..... Word2Vec Sample\n","  [ ] wordnet............. WordNet\n","  [ ] wordnet_ic.......... WordNet-InfoContent\n","  [ ] words............... Word Lists\n","  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n","                           English Prose\n","\n","Collections:\n","  [P] all-corpora......... All the corpora\n","  [P] all-nltk............ All packages available on nltk_data gh-pages\n","                           branch\n","  [P] all................. All packages\n","  [P] book................ Everything used in the NLTK Book\n","  [P] popular............. Popular packages\n","  [P] tests............... Packages for running tests\n","\n","([*] marks installed packages; [P] marks partially installed collections)\n","\n","Download which package (l=list; x=cancel)?\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UDYaAoSwMdRI","colab_type":"code","colab":{}},"source":["!pip install gensim\n","!pip install umap-learn\n","!pip install wikipedia\n","!pip install unidecode"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDiS_jPuMdRQ","colab_type":"code","colab":{}},"source":["import re\n","import nltk\n","from nltk.util import ngrams\n","from nltk.corpus import stopwords\n","import wikipedia\n","import string\n","from unidecode import unidecode\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd\n","import urllib.request\n","import bz2\n","import gensim\n","import warnings\n","import numpy as np\n","from gensim.models import word2vec\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","import umap\n","warnings.filterwarnings('ignore')\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBIF_aV4MdRV","colab_type":"text"},"source":["# Definição do Corpus"]},{"cell_type":"markdown","metadata":{"id":"IX6bKXbFMdRX","colab_type":"text"},"source":["## Base"]},{"cell_type":"code","metadata":{"id":"i7omAZSRMdRY","colab_type":"code","colab":{}},"source":["wikipedia.set_lang(\"pt\")\n","bh = wikipedia.page(\"Belo Horizonte\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fEnBfNMMdRd","colab_type":"code","colab":{}},"source":["corpus = bh.content"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frMByOQlMdRj","colab_type":"text"},"source":["Selecionamos algumas frases do corpus de BH da wikipedia.\n","\n","Conside a lista abaixo como nosso corpus de documentos. Cada elemento da lista, considere como um único documento."]},{"cell_type":"code","metadata":{"id":"PCz3ntY0MdRl","colab_type":"code","colab":{}},"source":["documentos = \\\n","[\"Belo Horizonte é um município brasileiro e a capital do estado de Minas Gerais\",\n","\"A populacao de Belo Horizonte é estimada em 2 501 576 habitantes, conforme estimativas do Instituto Brasileiro de Geografia e Estatística\",\n","\"Belo Horizonte já foi indicada pelo Population Crisis Commitee, da ONU, como a metrópole com melhor qualidade de vida na América Latina\",\n","\"Belo Horizonte é mundialmente conhecida e exerce significativa influência nacional e até internacional, seja do ponto de vista cultural, econômico ou político\",\n","\"Belo Horizonte é a capital do segundo estado mais populoso do Brasil, Minas Gerais\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jfXhioJtMdRu","colab_type":"text"},"source":["## Preprocessamento"]},{"cell_type":"markdown","metadata":{"id":"La6obq5WMdRv","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","1) Escreva uma método que realiza o pré-processamento da lista de <b>documentos</b>.\n","\n","O método deve, para cada documento:\n","- tokenizar cada palavra\n","- remover stopwords\n","- remover números\n","- remover pontuções\n","- remover acentos"]},{"cell_type":"code","metadata":{"id":"EpwvPhrQMdRy","colab_type":"code","colab":{}},"source":["def pre_processamento_texto(corpus):\n","    return corpus_alt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jnfjg2QmMdR4","colab_type":"code","colab":{}},"source":["corpus_processado = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qq1oim8GMdR-","colab_type":"text"},"source":["# Representação Textual"]},{"cell_type":"markdown","metadata":{"id":"KrtxSt2xMdR_","colab_type":"text"},"source":["## N-grams\n","\n","Existem várias formas de calcular os N-grams.\n","\n","Iremos estudar o: Phrases do gensim e o ngrams do NLTK"]},{"cell_type":"markdown","metadata":{"id":"ZscaHBnFMdSA","colab_type":"text"},"source":["### NLTK\n","\n","Para retornar o ngrams do nltk, utilize o exemplo abaixo:\n","\n","```python\n","list(ngrams(corpus, 2)\n","```\n","\n","<b> Atividade </b>\n","\n","2) Faça um código para armazena os bigrams de cada documento do corpus <b>corpus_processado</b> na variável corpus_ngrams_nltk.\n","Depois imprima os bigramas de cada documento"]},{"cell_type":"code","metadata":{"id":"Xgs1BE_1MdSC","colab_type":"code","colab":{}},"source":["corpus_ngrams_nltk = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQpWuv2TMdSH","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nzfi_ddLMdSL","colab_type":"text"},"source":["### Phrases - Gensim\n","\n","Forma mais inteligente de calcular os bigrams. Ela calcula os bigramas levando em consideração a frequência do par das palavaras em todos os documentos.\n","Para isso ele treina um modelo e depois aplica no corpus.\n","\n","```python\n","#treinamento bigrams\n","model_corpus_phrases = gensim.models.Phrases(corpus_processado, min_count=1)\n","#calulando os bigrams do corpus processado\n","bigram_corpus = model_corpus_phrases[corpus_processado]\n","```\n","\n","<b> Atividade </b>\n","\n","3) Faça um código que treine os bigrams, sendo que o <b>min_count = 1</b>. \n","O <b>min_count</b> é a contagem mínima que aquele par de palavras deve aparecer junto para considerarmos com um token. Teste também com outros valores de mim_count. Depois imprima os bigramas de cada documento.\n","Use o corpus_processado."]},{"cell_type":"code","metadata":{"id":"ZKPDLct-MdSM","colab_type":"code","colab":{}},"source":["model_corpus_phrases = \n","bigram_corpus = "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWtFJwWbMdSR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3dkEkQKfMdSV","colab_type":"text"},"source":["## TD-IDF\n","\n","Dica de leitura: https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XklQxnVKj7c\n","\n","Para representar o texto com TF-IDF utilizamos o TfidfVectorizer. A seguir apresentamos instruções sobre como utilizá-lo.\n","\n","```python\n","#primeiro criamos o objeto\n","vect = TfidfVectorizer()\n","vect #aqui você pode observa todos os parâmetros que o objeto possui\n","## Existem alguns parâmetros, opcionais, que podemos informar para uma possível melhora do nosso modelo. Por exemplo:\n","### inclui 1-grams e 2-grams\n","vect.set_params(ngram_range=(1, 2))\n","### ignora termos que a aparecem em mais de 50% dos documentoss\n","vect.set_params(max_df=0.5)\n","### só considero termos que aparecem em ao menos 2 documentos\n","vect.set_params(min_df=2)\n","\n","#depois aplicamos fit_transform para transformar o texto em números\n","docs_tdidf = vect.fit_transform(docs)\n","\n","#o docs_tdidf é uma matriz com os números que representam cada um dos textos. \n","## Conseguimos verificar a dimensão desta matriz:\n","print(docs_tdidf.shape)\n","\n","#Para visualizar as features capturadas pelo TF-IDF utilize:\n","print(vect.get_feature_names())\n","#Para visualizar os vetores correspondentes a cada palavara utilize:\n","df = pd.DataFrame(docs_tdidf.T.todense(), index=vect.get_feature_names(), columns=[\"doc\"+str(i+1) for i in range(0,len(docs))])\n","df\n","```"]},{"cell_type":"markdown","metadata":{"id":"Dg-1V2EMMdSW","colab_type":"text"},"source":["<b> Atividade: </b>\n","\n","4) Faça o TDIFTVectorizer nos documentos da variável <b>documentos</b> sem alterar nenhum parâmetro. "]},{"cell_type":"code","metadata":{"id":"8Vm-I90-MdSX","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPhXjtJrMdSb","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","5) Imprima o shape do resultado da atividade 4"]},{"cell_type":"code","metadata":{"id":"o0WCCZiHMdSc","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ehM671wqMdSl","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","6) Imprima as features capturadas em 4."]},{"cell_type":"code","metadata":{"id":"Ha-ihpYsMdSm","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWP94inzMdSs","colab_type":"text"},"source":["7) Imprima os vetores correspondentes a cada palavra de cada documento."]},{"cell_type":"code","metadata":{"id":"bfRhYkaNMdSt","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7v_tpXQMdSz","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","8) Depois repita os itens 4,5,6 e 7 aplicando a alteração de parâmetros"]},{"cell_type":"code","metadata":{"id":"GkSNRRgMMdS0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYVBV8zpMdS4","colab_type":"text"},"source":["## Bag of Words\n","\n","Para representar o bag of words utilizamos o CountVectorizer\n","\n","```python\n","#primeiro criamos o objeto\n","vect_bag = CountVectorizer(binary=True) #se binary = False -> ocorre a contagem da frequência em que a palavra aparece\n","vect_bag #imprime os parâmetros\n","\n","```\n","\n","<b> Atividade </b>\n","\n","9) Faça o CountVectorizer nos documentos da variável <b>documentos</b> considerando binary = True"]},{"cell_type":"code","metadata":{"id":"vlF5c5oQMdS5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8RCW0fFMdS9","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","10) Imprima o índice correspondente a cada token da lista retornada por vect_bag.get_feature_names()"]},{"cell_type":"code","metadata":{"id":"Xgb9Ar5KMdS-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkBJFIWVMdTC","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","11) Observe cada palavra do primeiro documento da vairável documents (documents[0]) e o vetor retornado para este mesmo documento \n","(docs_bag.todense()[0]).\n","\n","Com o resultado da célula anterior, check se as posições preenchidas com '1' são as posições que representam cada uma das palavras do docs[0].\n","Observe que é o bag of Words que vimos em sala."]},{"cell_type":"code","metadata":{"id":"C-Fo9ROUMdTC","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nl27KO7MdTH","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Wx3V_9HMdTL","colab_type":"text"},"source":["## Embedding"]},{"cell_type":"markdown","metadata":{"id":"8bmLhE4pMdTM","colab_type":"text"},"source":["### Utilizando um embedding treinado"]},{"cell_type":"markdown","metadata":{"id":"aKHhYZqnMdTN","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","12) Faça download do seguinte arquivo, realize a leitura deste arquivo e carregue o modelo: \n","https://drive.google.com/open?id=1zI8pGfbUHuU_0wY_FV4tD6w6ZCUJTQbh\n","\n","\n","O código abaixo é um exmplo de como ler o arquivo que você fez download.\n","\n","```python\n","newfilepath = \"embedding_wiki_100d_pt.txt\"\n","filepath = \"ptwiki_20180420_100d.txt.bz2\"\n","with open(newfilepath, 'wb') as new_file, bz2.BZ2File(filepath, 'rb') as file:\n","    for data in iter(lambda : file.read(100 * 1024), b''):\n","        new_file.write(data)\n","        \n","#carregar\n","word_vectors = gensim.models.KeyedVectors.load_word2vec_format(newfilepath, binary=False)\n","```\n","\n","<b> Dica </b> Use %%time para capturar o tempo de execução em cada célula."]},{"cell_type":"code","metadata":{"id":"k4sXXXbtMdTO","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xMCKo6ISMdTV","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","13) Imprima os vetores das palavras \"nlp\" e \"computacao\"\n","\n","```python\n","#exemplo de retorno do vetor\n","word_vectors[__]\n","```"]},{"cell_type":"code","metadata":{"id":"gR0QZQmbMdTW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1yszDZi2MdTb","colab_type":"text"},"source":["<b> Similaridade de Vetores </b> \n","\n","No gensim é possível realizar a similaridade utilizando o seguinte método:\n","\n","```python\n","word_vectors.most_similar(___)\n","```\n","\n","<b> Atividade </b>\n","\n","14) Verifique a similaridade das seguintes palavras: elizabete, raiva, segunda, dois, computação."]},{"cell_type":"code","metadata":{"id":"umskr0jYMdTb","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_KrE-cy5MdTg","colab_type":"text"},"source":["<b> Operação Vetorial</b>\n","\n","Na aula teórica, estudamos sobre as operação entre os vetores. \n","Agora vamos ver na prática:\n","\n","```python\n","#exemplo:\n","word_vectors.wv.most_similar(positive=['mulher', 'rei'], negative=['homem'], topn=10)\n","```\n","\n","<b>Atividade</b>\n","\n","15) Execute o exemplo acima em uma célula e repita para os seguintes cenários:\n","\n","- menino, menina, homem\n","- caminhada, andar, correr\n","- filho, filha, irmã\n","- pai, mãe, avô\n","\n","<b> Reflita </b> as palavras similares fazem sentido?"]},{"cell_type":"code","metadata":{"id":"PhR-n_V4MdTh","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wG8OdC2oMdTm","colab_type":"text"},"source":["<b> Similaridade Palavras </b>\n","\n","```python\n","#exemplo:\n","word_vectors.wv.similarity('mulher', 'homem')\n","```\n","\n","<b> Atividade </b>\n","\n","16) Verifique a similaridade entre as seguintes palavras:\n","\n","- mulher, homem\n","- homem, mulher\n","- computação, computacao\n","- londres, homem"]},{"cell_type":"code","metadata":{"id":"4S2hSfBDMdTn","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ir7FeO9UMdTt","colab_type":"text"},"source":["### Treinando seu embedding\n","\n","Aqui vamos utilizar o corpus machado. São textos/contos escritos por Machado de Assis.\n","Esse corpus é diponibilizado pelo NLTK."]},{"cell_type":"code","metadata":{"id":"64JrAsh9MdTu","colab_type":"code","colab":{}},"source":["from nltk.corpus import machado"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JaivC88uMdTz","colab_type":"code","colab":{}},"source":["machado.fileids()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHWaWKrjMdT5","colab_type":"code","colab":{}},"source":["raw_casmurro = machado.raw('contos/macn001.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L86mhVgeMdT8","colab_type":"code","colab":{}},"source":["raw_casmurro"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95CCG8lhMdUB","colab_type":"text"},"source":["O método ''machado_sents()'' retorna todo o texto quebrado pelas setenças e já tokenizado.\n","\n","As sentenças são separadas pelo \"\\n\". Dentro de cada sentença, divide os tokens separadas pelo espaço."]},{"cell_type":"code","metadata":{"id":"MFjN07rvMdUC","colab_type":"code","colab":{}},"source":["machado_sents = machado.sents()\n","print(machado_sents)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rw0GWniXMdUJ","colab_type":"text"},"source":["Vamos relizar um pré-processamento mínimo nos dados. Lembrando que: o pré-processamento é impotatíssimo no resultado final.\n","\n","<b> Atividade </b>\n","\n","17) Aplique as técnicas abaixo no documento <b> machado_sents</b>: \n","\n","- lower\n","- remoção pontuações"]},{"cell_type":"code","metadata":{"id":"v7uwThZ_MdUK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oswsk8NJMdUP","colab_type":"text"},"source":["<b> Treinando o embedding </b> \n","\n","Para treinar os embeddings existem alguns parâmetros, vide exemplo abaixo:\n","\n","```python\n","#Alguns parâmetros:\n","## size -> dimensão vetor\n","## min_count -> ignora todas palavras cuja frequência mínima é menor que este\n","## workers -> quantas threads serão utilizadas para treinar o modelo\n","## seed -> seed para geração do numero aleatório. \n","## sg -> 1 para skip-gram; caso contrário CBOW.\n","## window -> contexto, Distância máxima entre a palavra atual e a prevista em uma frase. O default é 5.\n","model = word2vec.Word2Vec(text_preproc, min_count=10, workers=4, seed=123, sg=1, size=300, window=5)\n","```\n","\n","<b> Atividade </b>\n","\n","18) Gere os embeddings com o texto processado do documento de Machado de Assis.\n"]},{"cell_type":"code","metadata":{"id":"1NyH9AS-MdUQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bl9wgOkzMdUU","colab_type":"text"},"source":["\n","<b> Atividade </b>\n","\n","19) Faça os itens abaixo:\n","\n","- Verifique o vetor de embeddings da variável \"dom\"\n","- Verifique a similaridade entre \"mulher\" e \"homem\"\n","- Verifique a similaridade entre \"dom\" e \"casmurro\""]},{"cell_type":"code","metadata":{"id":"4NsalWifMdUV","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"khHZsHyZMdUc","colab_type":"text"},"source":["<b> Salvando o modelo</b>"]},{"cell_type":"code","metadata":{"id":"pz05xpcWMdUd","colab_type":"code","colab":{}},"source":["model.wv.save_word2vec_format('model_emb_treinado.bin', binary=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EqUG0t3LMdUj","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","Dada as seguintes palavras:\n","\n","foi, relógio, amor, raiva, brasil.\n","\n","20) Escreva um método que retorne uma lista com as 5 palavras similares de cada uma das listadas anteriormente.\n","Imprima a lista das palavras similares, incluindo a palavra origem."]},{"cell_type":"code","metadata":{"id":"QNEBB7-6MdUl","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iGpTFt7nMdUr","colab_type":"text"},"source":["### Visualização"]},{"cell_type":"markdown","metadata":{"id":"Rsz6b50fMdUs","colab_type":"text"},"source":["Para a visualização dos embeddings iremos  construir um array com todas as palavras retornadas anteriormente.\n","\n","<b> Atividade </b>\n","\n","21) Primeiro, gere uma única lista com todas as palavras retornadas anteriomente. O array deve ter tamanho 30."]},{"cell_type":"code","metadata":{"id":"2k-NF5DaMdUt","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"veKjoHfSMdU5","colab_type":"text"},"source":["22) Gere um array com todos os embeddings das palavras anteriores. Este terá dimensão (30,300)"]},{"cell_type":"code","metadata":{"id":"PbvMKXLmMdU6","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sJmCHiK3MdU9","colab_type":"text"},"source":["<b>Dica: </b> Use a função abaixo para plotar o array 2D que será gerado com o método PCA, TSNE e UMAP"]},{"cell_type":"code","metadata":{"id":"6wxtuUNWMdU-","colab_type":"code","colab":{}},"source":["def plot_embedding_2d(array_2d, all_words, words_seed):\n","    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n","    for (x, y), w in zip(array_2d, all_words):\n","        ax.scatter(x, y, c='red' if w in words_seed else 'blue')\n","        ax.annotate(w,\n","                     xy=(x, y),\n","                     xytext=(5, 2),\n","                     textcoords='offset points',\n","                     ha='right',\n","                     va='bottom')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qATHKSu5MdVD","colab_type":"text"},"source":["#### PCA\n","\n","<b> É uma ténica que existe a mais de século. É rápido, determinístico e linear. Essa linearidade limita sua utilidade em domínios complexos, como linguagem natural ou imagens, onde a estrutura não linear. </b>\n","\n","Mais informações: https://medium.com/towards-artificial-intelligence/machine-learning-dimensionality-reduction-via-principal-component-analysis-1bdc77462831\n","\n","\n","<b> Atividade </b>\n","\n","23) Gere a visualização dos embeddings anteriores utilizando o PCA para reduzir a dimensionalidade.\n","\n","Exemplo do PCA:\n","\n","```python\n","#uso de PCA\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(array_embeddings)\n","```"]},{"cell_type":"code","metadata":{"id":"blsxrot8MdVE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40udJCqFMdVO","colab_type":"text"},"source":["#### TSNE\n","<b> Uma técnica mais recente que captura estrutura não linear é o t-SNE, que significa distribuição estocástica de embedding viziznhos em t ( t-distributed Stochastic Neighbor Embedding). \n","É uma ótima técnica para capturar a estrutura não linear em dados de alta dimensão(pelo menos em nível local). Isto é, dois pontos que são próximos no espaço de alta dimensão a probabilidade de estarem próximos em uma dimensão baixa é alta. </b>\n","\n","Mais informações: https://medium.com/@garora039/dimensionality-reduction-using-t-sne-effectively-cabb2cd519b\n","\n","<b> Atividade </b>\n","\n","24) Gere a visualização dos embeddings anteriores utilizando o TSNE para reduzir a dimensionalidade.\n","\n","Exemplo do TSNE:\n","\n","```python\n","#uso de TSNE\n","tsne = TSNE(n_components=2, random_state=0)\n","tsne_result =  tsne.fit_transform(array_embeddings)\n","```"]},{"cell_type":"code","metadata":{"id":"QvSWY6dZMdVO","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7H-hkDclMdVS","colab_type":"text"},"source":["#### UMAP\n","\n","<b> Técnica super nova!! Foi lançada em 2018! Observe que o resultado do UMAP com o TSNE é semelhante. Mas existem várias vantagens do UMAP, por exemplo: é mais rápido que o t-SNE; ele captura melhor a estrutura global </b>\n","\n","Mais informações: https://medium.com/@dan.allison/dimensionality-reduction-with-umap-b081837354dd\n","\n","<b> Atividade </b>\n","\n","25) Gere a visualização dos embeddings anteriores utilizando o UMAP para reduzir a dimensionalidade.\n","\n","Exemplo do UMAP:\n","\n","```python\n","#uso de TSNE\n","umap = umap.UMAP()\n","umap_result =  umap.fit_transform(array_embeddings)\n","```"]},{"cell_type":"code","metadata":{"id":"zs86CBqWMdVT","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVPEppl99NdV","colab_type":"text"},"source":["<b> Atividade </b>\n","\n","26) Faça uma comparação analisando os método: PCA, TSNE e Umap. Depois da sua análise, informe se teve algum com melhor desempenho."]},{"cell_type":"code","metadata":{"id":"hlWNQC__9NJh","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}