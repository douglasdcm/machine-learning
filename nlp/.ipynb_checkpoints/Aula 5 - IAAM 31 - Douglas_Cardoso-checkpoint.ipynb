{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /home/douglas/.local/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/lib/python3/dist-packages (from wikipedia) (2.22.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/douglas/.local/lib/python3.8/site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /home/douglas/.local/lib/python3.8/site-packages (from beautifulsoup4->wikipedia) (2.0.1)\n",
      "Collecting networkx\n",
      "  Using cached networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/douglas/.local/lib/python3.8/site-packages (from networkx) (4.4.2)\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wikipedia\n",
    "!pip3 install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vzhN-4y0IxMX"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import operator\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/douglas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGbzFkWDIxMe"
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F7r_zQ0IxMf"
   },
   "source": [
    "\n",
    "<b> Atividades </b>\n",
    "\n",
    "1) Faça download dos arquivos no link: https://drive.google.com/open?id=1PgGJ2OBFSKnr3O0RIgO2EvgbLh_avRJN\n",
    "\n",
    "2) Carregue os quatro documentos. Para carregar um arquivo ``yml``, siga o exemplo abaixo:\n",
    "```python\n",
    "with open('data/chats/file.yml') as file:\n",
    "    chat = yaml.load(file)\n",
    "```\n",
    "<b> Dica: adicione os 4 documentos em um único dicionário </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = [\"linguistic_knowledge\", \"money\", \"proverbs\", \"games\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "J3hXrzGGIxMg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-5cd51610a7d1>:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  conversas = yaml.load(file)\n"
     ]
    }
   ],
   "source": [
    "todos_chats = {}\n",
    "for arquivo in arquivos:\n",
    "    with open(arquivo+'.yml') as file:\n",
    "        conversas = yaml.load(file)\n",
    "        todos_chats[conversas[\"categories\"][0]] = conversas[\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conhecimento linguístico', 'money', 'proverbs', 'games'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todos_chats.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7H68OXiIxMk"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "A variável ``chat`` é um dicionário, que possui a chave ``conversations``, que contém várias listas. Dentro de cada lista, existe um par de frases, uma escrita por um humano e outra escrita por um robô. Neste exercício, só iremos considerar a frase do humano, ou seja, a primeira posição do par nas listas.\n",
    "Exemplo:\n",
    "\n",
    "```python\n",
    "chat = {'categories': ['compliment'],\n",
    "        'conversations': [['você é linda', 'eu sei'],\n",
    "                          ['adoro você', 'também'],\n",
    "                          ['gosto da sua sinceridade', 'que bom']]}\n",
    "```\n",
    "No exemplo acima, iremos considerar apenas as frases: <b> 'você é linda', 'adoro você' e 'gosto da sua sinceridade' </b>.\n",
    "\n",
    "Também iremos representar os chats por uma única lista. Ou seja, as listas de frases selecionadas no exemplo anterior se tornarão uma única lista: <b> ['você é linda adoro você e gosto da sua sinceridade'] </b>\n",
    "\n",
    "\n",
    "\n",
    "3) Conforme explicado anteriormente, faça as alterações necessárias no dado. Primeiro, considere apenas a frase da primeira posição, depois una todas as listas de frases em uma única lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iCfScHVGIxMl"
   },
   "outputs": [],
   "source": [
    "for k in todos_chats.keys():\n",
    "    todos_chats[k] = ' '.join([chat[0] for chat in todos_chats[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conhecimento linguístico': 'o que é linguística? o que é um verbo? estuda quem foi Saussure? O que é uma Gramática O que é um substantivo? quem é Noam Chomsky? o que é um sujeito? Quem é Marcos Bagno? Quais os livros que todo estudante de Letras deve ter? Quais as áreas da Linguística? Quando uso o porque, o por que, o porquê e o porquê? Pode explicar o uso dos porquês? Qual a forma correta, concerteza ou com certeza? O que são as figuras de linguagem? o que é um pronome? o que é a ANPOLL? Quando será realizado o próximo SIC? Quais os pronomes pessoais do caso reto? Quem é o autor de Análise de Textos de Comunicação? O que é um advérbio? O que é a langue? O que é a parole O que é letramento? O que é a alfabetização?',\n",
       " 'money': 'Você é pago mercado de ações mercado de ações mercado de ações mercado de ações mercado de ações mercado de ações mercado de ações mercado de ações taxa de juros o que é um dólar o que é dinheiro o que é o mercado de ações o que é o mercado de ações o que é o mercado de ações Qual é o seu investimento favorito? o que é economia o que é economia o que é economia eu faço estoque dinheiro quanto é que você ganha quanto é que você ganha quanto é que você ganha quanto você cobra quanto dinheiro você tem quanto dinheiro quanto dinheiro 1 dólar 1 real quem é o dono de um público',\n",
       " 'proverbs': 'Na casa do médico, todos estão doentes Ele insistiu tanto com ela que conseguiu casar-se Precisei pregar o prego, mas não tinha um martelo, então, preguei com uma pequena barra de ferro Ganhei o livro O Quinze, mas queria O Sagarana Sozinha não conseguirei concluir o trabalho Estava tudo combinado para a festa quando o pai dele chegou Você tem de ser paciente ao conversar com ela Cedo ou tarde a verdade vai aparecer Machocou e agora est· sendo machucado Melhor ter pouco que ambicionar muito e perder tudo Faça o trabalho devagar, mas bem feito Me desacatou, mas permaneci calada Depois que foi atropelado, só atravessa na faixa de pedestre com o farol fechado para os carros Ele fica lembrando da época que era piloto Quer bem feito, faça você mesmo! Se você continuar pisando na bola, vou ter de tomar uma providência! Mesmo sem saber direito como chegar ao evento, vou perguntando até achar Vou a agência de empregos amanhã de manhã Cuide da sua vida que eu cuido da minha Sou precavida! Ele não dispensa nada Tornou-se advogado, como o pai Ela merece perdão pelo seu erro Com as moedas que juntou no cofrinho, conseguiu comprar um carro zero Comprou uma impressora mais barata, mas deu defeito em dois meses O assunto veio ‡ tona, não foi por acaso Ela não pensa antes de falar e se denuncia a si mesma … preciso ter paciência para vencer Ela afirmou que nunca mais precisaria voltar· Minha namorada está me evitando, embora ela não confirme, sei que ela quer um tempo Finalmente consegui comprar aquele ingresso! Esqueci de levar o protetor solar, mas meu amigo trouxe e me emprestou. O celular era muito bonito na propaganda, mas ao vivo era muito ruim.',\n",
       " 'games': \"Você gosta de algum jogo? O que acha do Mario? O que acha do Sonic? O que acha do Pacman? O que acha do Assassin's Creed? O que acha do League of Legends? O que acha do Dota 2? Devo comprar um console? Mario ou Sonic? Conhece um programa chamado Discord? Conhece um programa chamado Team Speak? Conhece um programa chamado Skype? Você joga em consoles? Você acompanha o cenário de e-Sports? Você prefere PC ou console?\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todos_chats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kkAlOocIxMq"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "4) Faça uma limpeza nos dados carregados e processados do passo anterior. Aplique:\n",
    "- tokenização\n",
    "- lower\n",
    "- remoção stopwords\n",
    "- remoção pontuação\n",
    "\n",
    "Ao final você deve obter uma lista com todos os documentos e em cada documento as palavras tokenizadas, exemplo:\n",
    "\n",
    "```python\n",
    "[['oi', 'tudo', 'bem'],\n",
    " ['mario', 'game', 'jogo']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3ayE7l9DIxMs"
   },
   "outputs": [],
   "source": [
    "def pre_processamento_texto(corpus):\n",
    "\n",
    "    #tokenizacao\n",
    "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
    "    #capitalizacao\n",
    "    corpus_alt = [t.lower() for t in corpus_alt]\n",
    "    #remover stopwords\n",
    "    stopwords_ = stopwords.words(\"portuguese\")\n",
    "    corpus_alt = [t for t in corpus_alt if t not in stopwords_]\n",
    "    #remover numero\n",
    "    corpus_alt = [re.sub(r\"\\d\",\"\",t) for t in corpus_alt]\n",
    "    #remover pontuacoes\n",
    "    corpus_alt = [t for t in corpus_alt if t not in string.punctuation]\n",
    "\n",
    "    return corpus_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_chats_limpo = []\n",
    "for k in todos_chats.keys():\n",
    "    todos_chats_limpo.append(pre_processamento_texto(todos_chats[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6lfbAM2IxMx"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "5) Crie um dicionário com nosso corpus, de forma que cada termo único (valor) possua um índice (chave). \n",
    "\n",
    "Dica: \n",
    "- utilize o ```gensim.corpora.Dictionary ``` para criar este dicionário. \n",
    "- utilize o texto pré-processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_dtZTj-oIxMy"
   },
   "outputs": [],
   "source": [
    "dicionario = corpora.Dictionary(todos_chats_limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEA7Qh9gIxM2"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "6) Converta os documentos em índices, utilizando o método ```doc2bow``` em cada documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qqOLzcwLIxM3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['linguística',\n",
       "  'verbo',\n",
       "  'estuda',\n",
       "  'saussure',\n",
       "  'gramática',\n",
       "  'substantivo',\n",
       "  'noam',\n",
       "  'chomsky',\n",
       "  'sujeito',\n",
       "  'marcos',\n",
       "  'bagno',\n",
       "  'quais',\n",
       "  'livros',\n",
       "  'todo',\n",
       "  'estudante',\n",
       "  'letras',\n",
       "  'deve',\n",
       "  'ter',\n",
       "  'quais',\n",
       "  'áreas',\n",
       "  'linguística',\n",
       "  'uso',\n",
       "  'porque',\n",
       "  'porquê',\n",
       "  'porquê',\n",
       "  'pode',\n",
       "  'explicar',\n",
       "  'uso',\n",
       "  'porquês',\n",
       "  'forma',\n",
       "  'correta',\n",
       "  'concerteza',\n",
       "  'certeza',\n",
       "  'figuras',\n",
       "  'linguagem',\n",
       "  'pronome',\n",
       "  'anpoll',\n",
       "  'realizado',\n",
       "  'próximo',\n",
       "  'sic',\n",
       "  'quais',\n",
       "  'pronomes',\n",
       "  'pessoais',\n",
       "  'caso',\n",
       "  'reto',\n",
       "  'autor',\n",
       "  'análise',\n",
       "  'textos',\n",
       "  'comunicação',\n",
       "  'advérbio',\n",
       "  'langue',\n",
       "  'parole',\n",
       "  'letramento',\n",
       "  'alfabetização'],\n",
       " ['pago',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'taxa',\n",
       "  'juros',\n",
       "  'dólar',\n",
       "  'dinheiro',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'mercado',\n",
       "  'ações',\n",
       "  'investimento',\n",
       "  'favorito',\n",
       "  'economia',\n",
       "  'economia',\n",
       "  'economia',\n",
       "  'faço',\n",
       "  'estoque',\n",
       "  'dinheiro',\n",
       "  'quanto',\n",
       "  'ganha',\n",
       "  'quanto',\n",
       "  'ganha',\n",
       "  'quanto',\n",
       "  'ganha',\n",
       "  'quanto',\n",
       "  'cobra',\n",
       "  'quanto',\n",
       "  'dinheiro',\n",
       "  'quanto',\n",
       "  'dinheiro',\n",
       "  'quanto',\n",
       "  'dinheiro',\n",
       "  'dólar',\n",
       "  'real',\n",
       "  'dono',\n",
       "  'público'],\n",
       " ['casa',\n",
       "  'médico',\n",
       "  'todos',\n",
       "  'doentes',\n",
       "  'insistiu',\n",
       "  'tanto',\n",
       "  'conseguiu',\n",
       "  'casar',\n",
       "  'precisei',\n",
       "  'pregar',\n",
       "  'prego',\n",
       "  'martelo',\n",
       "  'então',\n",
       "  'preguei',\n",
       "  'pequena',\n",
       "  'barra',\n",
       "  'ferro',\n",
       "  'ganhei',\n",
       "  'livro',\n",
       "  'quinze',\n",
       "  'queria',\n",
       "  'sagarana',\n",
       "  'sozinha',\n",
       "  'conseguirei',\n",
       "  'concluir',\n",
       "  'trabalho',\n",
       "  'tudo',\n",
       "  'combinado',\n",
       "  'festa',\n",
       "  'pai',\n",
       "  'chegou',\n",
       "  'ser',\n",
       "  'paciente',\n",
       "  'conversar',\n",
       "  'cedo',\n",
       "  'tarde',\n",
       "  'verdade',\n",
       "  'vai',\n",
       "  'aparecer',\n",
       "  'machocou',\n",
       "  'agora',\n",
       "  'est',\n",
       "  '·',\n",
       "  'sendo',\n",
       "  'machucado',\n",
       "  'melhor',\n",
       "  'ter',\n",
       "  'pouco',\n",
       "  'ambicionar',\n",
       "  'perder',\n",
       "  'tudo',\n",
       "  'faça',\n",
       "  'trabalho',\n",
       "  'devagar',\n",
       "  'bem',\n",
       "  'feito',\n",
       "  'desacatou',\n",
       "  'permaneci',\n",
       "  'calada',\n",
       "  'atropelado',\n",
       "  'atravessa',\n",
       "  'faixa',\n",
       "  'pedestre',\n",
       "  'farol',\n",
       "  'fechado',\n",
       "  'carros',\n",
       "  'fica',\n",
       "  'lembrando',\n",
       "  'época',\n",
       "  'piloto',\n",
       "  'quer',\n",
       "  'bem',\n",
       "  'feito',\n",
       "  'faça',\n",
       "  'continuar',\n",
       "  'pisando',\n",
       "  'bola',\n",
       "  'vou',\n",
       "  'ter',\n",
       "  'tomar',\n",
       "  'providência',\n",
       "  'saber',\n",
       "  'direito',\n",
       "  'chegar',\n",
       "  'evento',\n",
       "  'vou',\n",
       "  'perguntando',\n",
       "  'achar',\n",
       "  'vou',\n",
       "  'agência',\n",
       "  'empregos',\n",
       "  'amanhã',\n",
       "  'manhã',\n",
       "  'cuide',\n",
       "  'vida',\n",
       "  'cuido',\n",
       "  'precavida',\n",
       "  'dispensa',\n",
       "  'nada',\n",
       "  'tornou',\n",
       "  'advogado',\n",
       "  'pai',\n",
       "  'merece',\n",
       "  'perdão',\n",
       "  'erro',\n",
       "  'moedas',\n",
       "  'juntou',\n",
       "  'cofrinho',\n",
       "  'conseguiu',\n",
       "  'comprar',\n",
       "  'carro',\n",
       "  'zero',\n",
       "  'comprou',\n",
       "  'impressora',\n",
       "  'barata',\n",
       "  'deu',\n",
       "  'defeito',\n",
       "  'dois',\n",
       "  'meses',\n",
       "  'assunto',\n",
       "  'veio',\n",
       "  '‡',\n",
       "  'tona',\n",
       "  'acaso',\n",
       "  'pensa',\n",
       "  'antes',\n",
       "  'falar',\n",
       "  'denuncia',\n",
       "  'si',\n",
       "  'mesma',\n",
       "  '…',\n",
       "  'preciso',\n",
       "  'ter',\n",
       "  'paciência',\n",
       "  'vencer',\n",
       "  'afirmou',\n",
       "  'nunca',\n",
       "  'precisaria',\n",
       "  'voltar',\n",
       "  '·',\n",
       "  'namorada',\n",
       "  'evitando',\n",
       "  'embora',\n",
       "  'confirme',\n",
       "  'sei',\n",
       "  'quer',\n",
       "  'tempo',\n",
       "  'finalmente',\n",
       "  'consegui',\n",
       "  'comprar',\n",
       "  'ingresso',\n",
       "  'esqueci',\n",
       "  'levar',\n",
       "  'protetor',\n",
       "  'solar',\n",
       "  'amigo',\n",
       "  'trouxe',\n",
       "  'emprestou',\n",
       "  'celular',\n",
       "  'bonito',\n",
       "  'propaganda',\n",
       "  'vivo',\n",
       "  'ruim'],\n",
       " ['gosta',\n",
       "  'algum',\n",
       "  'jogo',\n",
       "  'acha',\n",
       "  'mario',\n",
       "  'acha',\n",
       "  'sonic',\n",
       "  'acha',\n",
       "  'pacman',\n",
       "  'acha',\n",
       "  \"assassin's\",\n",
       "  'creed',\n",
       "  'acha',\n",
       "  'league',\n",
       "  'of',\n",
       "  'legends',\n",
       "  'acha',\n",
       "  'dota',\n",
       "  'devo',\n",
       "  'comprar',\n",
       "  'console',\n",
       "  'mario',\n",
       "  'sonic',\n",
       "  'conhece',\n",
       "  'programa',\n",
       "  'chamado',\n",
       "  'discord',\n",
       "  'conhece',\n",
       "  'programa',\n",
       "  'chamado',\n",
       "  'team',\n",
       "  'speak',\n",
       "  'conhece',\n",
       "  'programa',\n",
       "  'chamado',\n",
       "  'skype',\n",
       "  'joga',\n",
       "  'consoles',\n",
       "  'acompanha',\n",
       "  'cenário',\n",
       "  'sports',\n",
       "  'prefere',\n",
       "  'pc',\n",
       "  'console']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todos_chats_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bow = [dicionario.doc2bow(chat) for chat in todos_chats_limpo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhhrYOWxIxM8"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "7) Aplique o modelo LDA do Gensim nos dados processados acima. O modelo recebe a matriz de índices gerada anteriormente.\n",
    "Abaixo, apresentamos um exemplo de como aplicar o modelo.\n",
    "\n",
    "\n",
    "```python\n",
    "model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=4, id2word = dictionary, passes=1000, random_state=123)\n",
    "#para mostrar os tópicos:\n",
    "model.show_topics()\n",
    "```\n",
    "\n",
    "Fique a vontade para alterar os parâmetros e fazer testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "mzVfsXvxIxM9"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.ldamodel.LdaModel(doc_bow, num_topics=4, id2word = dicionario, passes=1000, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.099*\"ações\" + 0.099*\"mercado\" + 0.064*\"quanto\" + 0.046*\"dinheiro\" + 0.029*\"economia\" + 0.029*\"ganha\" + 0.020*\"dólar\" + 0.011*\"juros\" + 0.011*\"pago\" + 0.011*\"real\"'),\n",
       " (1,\n",
       "  '0.028*\"quais\" + 0.020*\"porquê\" + 0.020*\"uso\" + 0.020*\"linguística\" + 0.011*\"pronomes\" + 0.011*\"pronome\" + 0.011*\"porquês\" + 0.011*\"pode\" + 0.011*\"pessoais\" + 0.011*\"porque\"'),\n",
       " (2,\n",
       "  '0.015*\"ter\" + 0.015*\"vou\" + 0.010*\"comprar\" + 0.010*\"trabalho\" + 0.010*\"quer\" + 0.010*\"conseguiu\" + 0.010*\"tudo\" + 0.010*\"faça\" + 0.010*\"pai\" + 0.010*\"·\"'),\n",
       " (3,\n",
       "  '0.060*\"acha\" + 0.031*\"chamado\" + 0.031*\"programa\" + 0.031*\"conhece\" + 0.021*\"console\" + 0.021*\"sonic\" + 0.021*\"mario\" + 0.012*\"consoles\" + 0.012*\"devo\" + 0.012*\"creed\"')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq7LLzSiIxNB"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "8) Para responder esta questão utilize os parâmetros sugeridos na questão 7. Analise os grupos gerados anteriormente e responda:\n",
    "- a) Os grupos são bem dividos? Caracterize-os.\n",
    "- b) Dê um nome para cada grupo de tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTWhIiJ0IxNC"
   },
   "source": [
    "a) os grupos são bem dividos, mas o grupo que seria relativo à \"provérbios\" ficou caracterizado com algo parecido como \"aquisições\" ou \"posses\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMlGC2-5IxNF"
   },
   "source": [
    "d) tópico 1: finanças/mercado; tópico 2: linguagens; tópico 3: aquisições; tópico 4: jogos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGEqR-dCIxNJ"
   },
   "source": [
    "# Sumarização\n",
    "\n",
    "Aqui iremos realizar uma sumarização de notíticas com técnicas que vimos nas aulas anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uzi9WNp5IxNK"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "9) Faça download do texto da seguinte notícia: \n",
    "\n",
    "``https://g1.globo.com/sp/sao-paulo/noticia/2020/03/03/cai-para-130-numero-de-casos-suspeitos-de-coronavirus-no-estado-de-sao-paulo.ghtml``\n",
    "\n",
    "Dica: para fazer download, consulte a Aula Prática 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HEK_Re-6IxNL"
   },
   "outputs": [],
   "source": [
    "def match_class(target):                                                        \n",
    "    def do_match(tag):                                                          \n",
    "        classes = tag.get('class', [])                                          \n",
    "        return all(c in classes for c in target)                                \n",
    "    return do_match \n",
    "\n",
    "def get_text_url(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    #remove marcações de scripts e style\n",
    "    texto = soup.find_all(match_class([\"content-text__container\"]))\n",
    "    all_text = \"\"\n",
    "    for t in texto:\n",
    "        all_text += t.get_text()\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticia = get_text_url(\"https://g1.globo.com/sp/sao-paulo/noticia/2020/03/03/cai-para-130-numero-de-casos-suspeitos-de-coronavirus-no-estado-de-sao-paulo.ghtml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osxILn5uIxNP"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "10) Imprima o texto coletado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "jIZzS17MIxNQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A Secretaria Estadual de Saúde de São Paulo divulgou nesta terça-feira (3) que caiu o número de pacientes com suspeita de coronavírus no estado. Enquanto na segunda-feira (2) eram 163 casos suspeitos, nesta terça são 130.  Foram descartados, ao todo, 104 casos no estado, destes, 56 foram nas últimas 24 horas. Entretanto, segundo informou a pasta, entre segunda (2) e terça (3), 23 novos surgiram.  Permanecem confirmados apenas 2 casos da doença no país, ambos no estado de São Paulo. Os pacientes estão em quarentena domiciliar. Ambos estiveram em viagem na Itália.  \"Os dois confirmados estão evoluindo bem clinicamente, mas ainda têm alguns sintomas que fazem com que eles continuem em isolamento domiciliar\", salientou Paulo Menezes, coordenador do comitê de operações emergenciais (COE) da Secretaria Estadual de Saúde.  O número de pessoas que tiveram contato com o segundo caso confirmado não foi divulgado pela secretaria. No primeiro caso confirmado, eram 34 pessoas, entre passageiros do voo e familiares do paciente.  “Existem contactantes [do segundo caso], eles estão sendo monitorados, mas a gente não está divulgando mais números de contactantes por conta da privacidade das pessoas”, disse Paulo Menezes, coordenador do comitê de operações emergenciais (COE) do estado.  O secretário estadual de saúde destaca ainda que o segundo paciente confirmado foi cauteloso assim que percebeu os primeiros sintomas. “Ele foi muito cuidadoso colocou máscara antes do voo e foi pro hospital”, disse o secretário José Henrique Germann.  Participaram do evento também o coordenador do Centro de Contingenciamento de Emergências para o coronavírus do estado, o médico David Uip, o secretário estadual de saúde, José Henrique Germann, e Helena Sato, coordenadora do Centro de Vigilância Epidemiológica (CVE) estadual.  Segundo Uip, o laboratório Adolfo Lutz, onde os testes são realizados, irá \"qualificar outros laboratórios\" para realizar o exame.  \"De ontem pra hoje, nós descartamos 56 daqueles 163. Já tiveram seus exames negativos. E tivemos a inclusão de notificações de 23 novas pessoas. Então somamos 130 casos suspeitos. São todos viajantes. Os casos suspeitos que tínhamos de contactantes dos casos confirmados já foram descartados\", disse Menezes.  Segundo a metodologia da Secretaria Estadual de Saúde, para um caso ser considerado suspeito é necessário que o paciente tenha apresentado, além dos sintomas, histórico de viagem ou contato com caso suspeito.  No sábado (29), a Secretaria Estadual de Saúde e o Ministério da Saúde confirmaram o segundo caso de coronavírus no estado. Trata-se de um homem de 32 anos que reside em São Paulo e que chegou de Milão, na Itália, na quinta-feira (27).  Mesmo com a segunda confirmação, não há mudança da situação nacional, pois não existem evidências de circulação sustentada do vírus em território brasileiro.  A tendência é que, a partir dos próximos dias, o padrão de identificação dos casos suspeitos mude. Segundo Menezes, \"daqui a alguns dias nós vamos trabalhar por gravidade e não por procedência pra fazer a identificação dos casos suspeitos\".  \"Eu concordo com o ministério da Saúde: eu acho que estamos em um momento de pandemia há dias. Meu sentimento pessoal é de que estamos em um momento pandêmico\", salientou Uip.  A Organização Mundial de Saúde (OMS) é que define os critérios e se a doença é caracterizada ou não como pandemia.  O governo estadual lançou uma cartilha de orientação e prevenção do coronavírus em cinco idiomas - português, inglês, espanhol, italiano e chinês - na versão impressa e eletrônica. O estado irá liberar R$ 30 milhões em recursos para as ações de prevenção e informação.  Após a confirmação de um caso, o governo estadual anunciou a criação de um centro de contingência para monitorar casos de coronavírus no estado. A função do centro de contingência é coordenar ações contra a propagação do Covid-19. O grupo é presidido pelo infectologista David Uip e conta com profissionais do Instituto Butantan e médicos das redes pública e privada, sob a supervisão do Secretário de Estado da Saúde, José Henrique Germann.  Nesta quinta-feira (27), o governo federal anunciou que vai antecipar para 23 de março a Campanha Nacional de Vacinação contra a gripe – anteriormente, a abertura estava prevista para a segunda quinzena de abril.  A vacina contra a gripe não protege contra o novo coronavírus, mas, sim, contra tipos de influenza (família à qual pertence o H1N1, por exemplo). E justamente por isso pode ajudar profissionais de saúde a diagnosticar – por eliminação – eventuais casos de Covid-19.  Quem for viajar aos locais com circulação do vírus deve evitar contato com pessoas doentes, animais (vivos ou mortos), e a circulação em mercados de animais e seus produtos. '"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnQTLtE4IxNU"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "11) Faça a tokenização das sentenças."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "TLa2At5ZIxNU"
   },
   "outputs": [],
   "source": [
    "noticia_sents = sent_tokenize(noticia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' A Secretaria Estadual de Saúde de São Paulo divulgou nesta terça-feira (3) que caiu o número de pacientes com suspeita de coronavírus no estado.',\n",
       " 'Enquanto na segunda-feira (2) eram 163 casos suspeitos, nesta terça são 130.',\n",
       " 'Foram descartados, ao todo, 104 casos no estado, destes, 56 foram nas últimas 24 horas.',\n",
       " 'Entretanto, segundo informou a pasta, entre segunda (2) e terça (3), 23 novos surgiram.',\n",
       " 'Permanecem confirmados apenas 2 casos da doença no país, ambos no estado de São Paulo.',\n",
       " 'Os pacientes estão em quarentena domiciliar.',\n",
       " 'Ambos estiveram em viagem na Itália.',\n",
       " '\"Os dois confirmados estão evoluindo bem clinicamente, mas ainda têm alguns sintomas que fazem com que eles continuem em isolamento domiciliar\", salientou Paulo Menezes, coordenador do comitê de operações emergenciais (COE) da Secretaria Estadual de Saúde.',\n",
       " 'O número de pessoas que tiveram contato com o segundo caso confirmado não foi divulgado pela secretaria.',\n",
       " 'No primeiro caso confirmado, eram 34 pessoas, entre passageiros do voo e familiares do paciente.',\n",
       " '“Existem contactantes [do segundo caso], eles estão sendo monitorados, mas a gente não está divulgando mais números de contactantes por conta da privacidade das pessoas”, disse Paulo Menezes, coordenador do comitê de operações emergenciais (COE) do estado.',\n",
       " 'O secretário estadual de saúde destaca ainda que o segundo paciente confirmado foi cauteloso assim que percebeu os primeiros sintomas.',\n",
       " '“Ele foi muito cuidadoso colocou máscara antes do voo e foi pro hospital”, disse o secretário José Henrique Germann.',\n",
       " 'Participaram do evento também o coordenador do Centro de Contingenciamento de Emergências para o coronavírus do estado, o médico David Uip, o secretário estadual de saúde, José Henrique Germann, e Helena Sato, coordenadora do Centro de Vigilância Epidemiológica (CVE) estadual.',\n",
       " 'Segundo Uip, o laboratório Adolfo Lutz, onde os testes são realizados, irá \"qualificar outros laboratórios\" para realizar o exame.',\n",
       " '\"De ontem pra hoje, nós descartamos 56 daqueles 163.',\n",
       " 'Já tiveram seus exames negativos.',\n",
       " 'E tivemos a inclusão de notificações de 23 novas pessoas.',\n",
       " 'Então somamos 130 casos suspeitos.',\n",
       " 'São todos viajantes.',\n",
       " 'Os casos suspeitos que tínhamos de contactantes dos casos confirmados já foram descartados\", disse Menezes.',\n",
       " 'Segundo a metodologia da Secretaria Estadual de Saúde, para um caso ser considerado suspeito é necessário que o paciente tenha apresentado, além dos sintomas, histórico de viagem ou contato com caso suspeito.',\n",
       " 'No sábado (29), a Secretaria Estadual de Saúde e o Ministério da Saúde confirmaram o segundo caso de coronavírus no estado.',\n",
       " 'Trata-se de um homem de 32 anos que reside em São Paulo e que chegou de Milão, na Itália, na quinta-feira (27).',\n",
       " 'Mesmo com a segunda confirmação, não há mudança da situação nacional, pois não existem evidências de circulação sustentada do vírus em território brasileiro.',\n",
       " 'A tendência é que, a partir dos próximos dias, o padrão de identificação dos casos suspeitos mude.',\n",
       " 'Segundo Menezes, \"daqui a alguns dias nós vamos trabalhar por gravidade e não por procedência pra fazer a identificação dos casos suspeitos\".',\n",
       " '\"Eu concordo com o ministério da Saúde: eu acho que estamos em um momento de pandemia há dias.',\n",
       " 'Meu sentimento pessoal é de que estamos em um momento pandêmico\", salientou Uip.',\n",
       " 'A Organização Mundial de Saúde (OMS) é que define os critérios e se a doença é caracterizada ou não como pandemia.',\n",
       " 'O governo estadual lançou uma cartilha de orientação e prevenção do coronavírus em cinco idiomas - português, inglês, espanhol, italiano e chinês - na versão impressa e eletrônica.',\n",
       " 'O estado irá liberar R$ 30 milhões em recursos para as ações de prevenção e informação.',\n",
       " 'Após a confirmação de um caso, o governo estadual anunciou a criação de um centro de contingência para monitorar casos de coronavírus no estado.',\n",
       " 'A função do centro de contingência é coordenar ações contra a propagação do Covid-19.',\n",
       " 'O grupo é presidido pelo infectologista David Uip e conta com profissionais do Instituto Butantan e médicos das redes pública e privada, sob a supervisão do Secretário de Estado da Saúde, José Henrique Germann.',\n",
       " 'Nesta quinta-feira (27), o governo federal anunciou que vai antecipar para 23 de março a Campanha Nacional de Vacinação contra a gripe – anteriormente, a abertura estava prevista para a segunda quinzena de abril.',\n",
       " 'A vacina contra a gripe não protege contra o novo coronavírus, mas, sim, contra tipos de influenza (família à qual pertence o H1N1, por exemplo).',\n",
       " 'E justamente por isso pode ajudar profissionais de saúde a diagnosticar – por eliminação – eventuais casos de Covid-19.',\n",
       " 'Quem for viajar aos locais com circulação do vírus deve evitar contato com pessoas doentes, animais (vivos ou mortos), e a circulação em mercados de animais e seus produtos.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticia_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O3yN7eEIxNa"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "12) Aplique o pré-processamento nos dados:\n",
    "\n",
    "- tokenização\n",
    "- lower\n",
    "- remoção stopwords\n",
    "- remoção pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kSLNMw4xIxNc"
   },
   "outputs": [],
   "source": [
    "def pre_processamento_texto(corpus):\n",
    "\n",
    "    #tokenizacao\n",
    "    corpus_alt = re.findall(r\"\\w+(?:'\\w+)?|[^\\w\\s]\", corpus)\n",
    "    #capitalizacao\n",
    "    corpus_alt = [t.lower() for t in corpus_alt]\n",
    "    #remover stopwords\n",
    "    stopwords_ = stopwords.words(\"portuguese\")\n",
    "    corpus_alt = [t for t in corpus_alt if t not in stopwords_]\n",
    "    #remover numero\n",
    "    corpus_alt = [re.sub(r\"\\d\",\"\",t) for t in corpus_alt]\n",
    "    #remover pontuacoes\n",
    "    corpus_alt = [t for t in corpus_alt if t not in string.punctuation]\n",
    "\n",
    "    return corpus_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias_processado = [pre_processamento_texto(sent) for sent in noticia_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['secretaria',\n",
       "  'estadual',\n",
       "  'saúde',\n",
       "  'paulo',\n",
       "  'divulgou',\n",
       "  'nesta',\n",
       "  'terça',\n",
       "  'feira',\n",
       "  'caiu',\n",
       "  'número',\n",
       "  'pacientes',\n",
       "  'suspeita',\n",
       "  'coronavírus',\n",
       "  'estado'],\n",
       " ['enquanto', 'segunda', 'feira', 'casos', 'suspeitos', 'nesta', 'terça'],\n",
       " ['descartados', 'todo', 'casos', 'estado', 'destes', 'últimas', 'horas'],\n",
       " ['entretanto',\n",
       "  'segundo',\n",
       "  'informou',\n",
       "  'pasta',\n",
       "  'segunda',\n",
       "  'terça',\n",
       "  'novos',\n",
       "  'surgiram'],\n",
       " ['permanecem',\n",
       "  'confirmados',\n",
       "  'apenas',\n",
       "  'casos',\n",
       "  'doença',\n",
       "  'país',\n",
       "  'ambos',\n",
       "  'estado',\n",
       "  'paulo'],\n",
       " ['pacientes', 'quarentena', 'domiciliar'],\n",
       " ['ambos', 'viagem', 'itália'],\n",
       " ['dois',\n",
       "  'confirmados',\n",
       "  'evoluindo',\n",
       "  'bem',\n",
       "  'clinicamente',\n",
       "  'ainda',\n",
       "  'têm',\n",
       "  'alguns',\n",
       "  'sintomas',\n",
       "  'fazem',\n",
       "  'continuem',\n",
       "  'isolamento',\n",
       "  'domiciliar',\n",
       "  'salientou',\n",
       "  'paulo',\n",
       "  'menezes',\n",
       "  'coordenador',\n",
       "  'comitê',\n",
       "  'operações',\n",
       "  'emergenciais',\n",
       "  'coe',\n",
       "  'secretaria',\n",
       "  'estadual',\n",
       "  'saúde'],\n",
       " ['número',\n",
       "  'pessoas',\n",
       "  'contato',\n",
       "  'segundo',\n",
       "  'caso',\n",
       "  'confirmado',\n",
       "  'divulgado',\n",
       "  'secretaria'],\n",
       " ['primeiro',\n",
       "  'caso',\n",
       "  'confirmado',\n",
       "  'pessoas',\n",
       "  'passageiros',\n",
       "  'voo',\n",
       "  'familiares',\n",
       "  'paciente'],\n",
       " ['“',\n",
       "  'existem',\n",
       "  'contactantes',\n",
       "  'segundo',\n",
       "  'caso',\n",
       "  'sendo',\n",
       "  'monitorados',\n",
       "  'gente',\n",
       "  'divulgando',\n",
       "  'números',\n",
       "  'contactantes',\n",
       "  'conta',\n",
       "  'privacidade',\n",
       "  'pessoas',\n",
       "  '”',\n",
       "  'disse',\n",
       "  'paulo',\n",
       "  'menezes',\n",
       "  'coordenador',\n",
       "  'comitê',\n",
       "  'operações',\n",
       "  'emergenciais',\n",
       "  'coe',\n",
       "  'estado'],\n",
       " ['secretário',\n",
       "  'estadual',\n",
       "  'saúde',\n",
       "  'destaca',\n",
       "  'ainda',\n",
       "  'segundo',\n",
       "  'paciente',\n",
       "  'confirmado',\n",
       "  'cauteloso',\n",
       "  'assim',\n",
       "  'percebeu',\n",
       "  'primeiros',\n",
       "  'sintomas'],\n",
       " ['“',\n",
       "  'cuidadoso',\n",
       "  'colocou',\n",
       "  'máscara',\n",
       "  'antes',\n",
       "  'voo',\n",
       "  'pro',\n",
       "  'hospital',\n",
       "  '”',\n",
       "  'disse',\n",
       "  'secretário',\n",
       "  'josé',\n",
       "  'henrique',\n",
       "  'germann'],\n",
       " ['participaram',\n",
       "  'evento',\n",
       "  'coordenador',\n",
       "  'centro',\n",
       "  'contingenciamento',\n",
       "  'emergências',\n",
       "  'coronavírus',\n",
       "  'estado',\n",
       "  'médico',\n",
       "  'david',\n",
       "  'uip',\n",
       "  'secretário',\n",
       "  'estadual',\n",
       "  'saúde',\n",
       "  'josé',\n",
       "  'henrique',\n",
       "  'germann',\n",
       "  'helena',\n",
       "  'sato',\n",
       "  'coordenadora',\n",
       "  'centro',\n",
       "  'vigilância',\n",
       "  'epidemiológica',\n",
       "  'cve',\n",
       "  'estadual'],\n",
       " ['segundo',\n",
       "  'uip',\n",
       "  'laboratório',\n",
       "  'adolfo',\n",
       "  'lutz',\n",
       "  'onde',\n",
       "  'testes',\n",
       "  'realizados',\n",
       "  'irá',\n",
       "  'qualificar',\n",
       "  'outros',\n",
       "  'laboratórios',\n",
       "  'realizar',\n",
       "  'exame'],\n",
       " ['ontem', 'pra', 'hoje', 'descartamos', 'daqueles'],\n",
       " ['exames', 'negativos'],\n",
       " ['inclusão', 'notificações', 'novas', 'pessoas'],\n",
       " ['então', 'somamos', 'casos', 'suspeitos'],\n",
       " ['todos', 'viajantes'],\n",
       " ['casos',\n",
       "  'suspeitos',\n",
       "  'contactantes',\n",
       "  'casos',\n",
       "  'confirmados',\n",
       "  'descartados',\n",
       "  'disse',\n",
       "  'menezes'],\n",
       " ['segundo',\n",
       "  'metodologia',\n",
       "  'secretaria',\n",
       "  'estadual',\n",
       "  'saúde',\n",
       "  'caso',\n",
       "  'ser',\n",
       "  'considerado',\n",
       "  'suspeito',\n",
       "  'necessário',\n",
       "  'paciente',\n",
       "  'apresentado',\n",
       "  'além',\n",
       "  'sintomas',\n",
       "  'histórico',\n",
       "  'viagem',\n",
       "  'contato',\n",
       "  'caso',\n",
       "  'suspeito'],\n",
       " ['sábado',\n",
       "  'secretaria',\n",
       "  'estadual',\n",
       "  'saúde',\n",
       "  'ministério',\n",
       "  'saúde',\n",
       "  'confirmaram',\n",
       "  'segundo',\n",
       "  'caso',\n",
       "  'coronavírus',\n",
       "  'estado'],\n",
       " ['trata',\n",
       "  'homem',\n",
       "  'anos',\n",
       "  'reside',\n",
       "  'paulo',\n",
       "  'chegou',\n",
       "  'milão',\n",
       "  'itália',\n",
       "  'quinta',\n",
       "  'feira'],\n",
       " ['segunda',\n",
       "  'confirmação',\n",
       "  'mudança',\n",
       "  'situação',\n",
       "  'nacional',\n",
       "  'pois',\n",
       "  'existem',\n",
       "  'evidências',\n",
       "  'circulação',\n",
       "  'sustentada',\n",
       "  'vírus',\n",
       "  'território',\n",
       "  'brasileiro'],\n",
       " ['tendência',\n",
       "  'partir',\n",
       "  'próximos',\n",
       "  'dias',\n",
       "  'padrão',\n",
       "  'identificação',\n",
       "  'casos',\n",
       "  'suspeitos',\n",
       "  'mude'],\n",
       " ['segundo',\n",
       "  'menezes',\n",
       "  'daqui',\n",
       "  'alguns',\n",
       "  'dias',\n",
       "  'vamos',\n",
       "  'trabalhar',\n",
       "  'gravidade',\n",
       "  'procedência',\n",
       "  'pra',\n",
       "  'fazer',\n",
       "  'identificação',\n",
       "  'casos',\n",
       "  'suspeitos'],\n",
       " ['concordo', 'ministério', 'saúde', 'acho', 'momento', 'pandemia', 'dias'],\n",
       " ['sentimento', 'pessoal', 'momento', 'pandêmico', 'salientou', 'uip'],\n",
       " ['organização',\n",
       "  'mundial',\n",
       "  'saúde',\n",
       "  'oms',\n",
       "  'define',\n",
       "  'critérios',\n",
       "  'doença',\n",
       "  'caracterizada',\n",
       "  'pandemia'],\n",
       " ['governo',\n",
       "  'estadual',\n",
       "  'lançou',\n",
       "  'cartilha',\n",
       "  'orientação',\n",
       "  'prevenção',\n",
       "  'coronavírus',\n",
       "  'cinco',\n",
       "  'idiomas',\n",
       "  'português',\n",
       "  'inglês',\n",
       "  'espanhol',\n",
       "  'italiano',\n",
       "  'chinês',\n",
       "  'versão',\n",
       "  'impressa',\n",
       "  'eletrônica'],\n",
       " ['estado',\n",
       "  'irá',\n",
       "  'liberar',\n",
       "  'r',\n",
       "  'milhões',\n",
       "  'recursos',\n",
       "  'ações',\n",
       "  'prevenção',\n",
       "  'informação'],\n",
       " ['após',\n",
       "  'confirmação',\n",
       "  'caso',\n",
       "  'governo',\n",
       "  'estadual',\n",
       "  'anunciou',\n",
       "  'criação',\n",
       "  'centro',\n",
       "  'contingência',\n",
       "  'monitorar',\n",
       "  'casos',\n",
       "  'coronavírus',\n",
       "  'estado'],\n",
       " ['função',\n",
       "  'centro',\n",
       "  'contingência',\n",
       "  'coordenar',\n",
       "  'ações',\n",
       "  'contra',\n",
       "  'propagação',\n",
       "  'covid'],\n",
       " ['grupo',\n",
       "  'presidido',\n",
       "  'infectologista',\n",
       "  'david',\n",
       "  'uip',\n",
       "  'conta',\n",
       "  'profissionais',\n",
       "  'instituto',\n",
       "  'butantan',\n",
       "  'médicos',\n",
       "  'redes',\n",
       "  'pública',\n",
       "  'privada',\n",
       "  'sob',\n",
       "  'supervisão',\n",
       "  'secretário',\n",
       "  'estado',\n",
       "  'saúde',\n",
       "  'josé',\n",
       "  'henrique',\n",
       "  'germann'],\n",
       " ['nesta',\n",
       "  'quinta',\n",
       "  'feira',\n",
       "  'governo',\n",
       "  'federal',\n",
       "  'anunciou',\n",
       "  'vai',\n",
       "  'antecipar',\n",
       "  'março',\n",
       "  'campanha',\n",
       "  'nacional',\n",
       "  'vacinação',\n",
       "  'contra',\n",
       "  'gripe',\n",
       "  '–',\n",
       "  'anteriormente',\n",
       "  'abertura',\n",
       "  'prevista',\n",
       "  'segunda',\n",
       "  'quinzena',\n",
       "  'abril'],\n",
       " ['vacina',\n",
       "  'contra',\n",
       "  'gripe',\n",
       "  'protege',\n",
       "  'contra',\n",
       "  'novo',\n",
       "  'coronavírus',\n",
       "  'sim',\n",
       "  'contra',\n",
       "  'tipos',\n",
       "  'influenza',\n",
       "  'família',\n",
       "  'pertence',\n",
       "  'hn',\n",
       "  'exemplo'],\n",
       " ['justamente',\n",
       "  'pode',\n",
       "  'ajudar',\n",
       "  'profissionais',\n",
       "  'saúde',\n",
       "  'diagnosticar',\n",
       "  '–',\n",
       "  'eliminação',\n",
       "  '–',\n",
       "  'eventuais',\n",
       "  'casos',\n",
       "  'covid'],\n",
       " ['viajar',\n",
       "  'locais',\n",
       "  'circulação',\n",
       "  'vírus',\n",
       "  'deve',\n",
       "  'evitar',\n",
       "  'contato',\n",
       "  'pessoas',\n",
       "  'doentes',\n",
       "  'animais',\n",
       "  'vivos',\n",
       "  'mortos',\n",
       "  'circulação',\n",
       "  'mercados',\n",
       "  'animais',\n",
       "  'produtos']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias_processado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7-clsbXIxNg"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "13) Agora iremos implementar a sumarização. Para sumarizar você precisará implementar um método para cada uma das orientações abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w0ojD9zIxNg"
   },
   "source": [
    "13.1) Faça um método que calcule a similaridade entre duas sentenças utilizando a similaridade do conseno. \n",
    "\n",
    "Dica: faça a tranformação aplicando Bag of Words. Consulte a Aula 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "q-TMDvqhIxNh"
   },
   "outputs": [],
   "source": [
    "palavras_unicas = (set(noticias_processado[0] + noticias_processado[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(binary=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(palavras_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = noticias_processado[0]\n",
    "sent2 = noticias_processado[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_s1 = model.transform([' '.join(sent1)]).todense()\n",
    "vetor_s2 = model.transform([' '.join(sent2)]).todense()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caiu',\n",
       " 'casos',\n",
       " 'coronavírus',\n",
       " 'divulgou',\n",
       " 'enquanto',\n",
       " 'estado',\n",
       " 'estadual',\n",
       " 'feira',\n",
       " 'nesta',\n",
       " 'número',\n",
       " 'pacientes',\n",
       " 'paulo',\n",
       " 'saúde',\n",
       " 'secretaria',\n",
       " 'segunda',\n",
       " 'suspeita',\n",
       " 'suspeitos',\n",
       " 'terça']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3030457633656632"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vetor_s1, vetor_s2).reshape(-1)[0]\n",
    "#cosine_similarity(vetor_s1, vetor_s2).reshape[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similaridade(s1, s2):\n",
    "    palavras_unicas = (set(s1 + s2))\n",
    "    model = CountVectorizer(binary=True)\n",
    "    model.fit(palavras_unicas)\n",
    "    vetor_s1 = model.transform([' '.join(s1)]).todense()\n",
    "    vetor_s2 = model.transform([' '.join(s2)]).todense()    \n",
    "    return cosine_similarity(vetor_s1, vetor_s2).reshape(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similaridade(noticias_processado[0], noticias_processado[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixH4xxUmIxNm"
   },
   "source": [
    "13.2) Faça um método para construir uma matriz de similaridades entre as sentenças. \n",
    "\n",
    "Exemplo, seu texto tem 10 sentenças, você deve construir uma matriz (10,10). A posição (0,1) da matriz corresponderá a similaridade do cosseno entre a sentença 0 e a sentença 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "kiLNeKlkIxNm"
   },
   "outputs": [],
   "source": [
    "qtde_sents = len(noticia_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_similaridades = np.zeros((len(noticia_sents) , len(noticia_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3030457633656632"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similaridade(noticias_processado[0], noticias_processado[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in range(0, qtde_sents):\n",
    "    for coluna in range(0, qtde_sents):\n",
    "        if linha == coluna:\n",
    "            continue\n",
    "        vetor_similaridades[linha][coluna] = similaridade(noticias_processado[linha], noticias_processado[coluna])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_similarides(sents):\n",
    "    qtde_sents = len(noticia_sents)\n",
    "    vetor_similaridades = np.zeros((len(noticia_sents) , len(noticia_sents)))\n",
    "    for linha in range(0, qtde_sents):\n",
    "        for coluna in range(0, qtde_sents):\n",
    "            if linha == coluna:\n",
    "                continue\n",
    "            vetor_similaridades[linha][coluna] = similaridade(noticias_processado[linha], noticias_processado[coluna])\n",
    "    \n",
    "    return vetor_similaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_similaridades = matriz_similarides(noticias_processado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.30304576, 0.10101525, ..., 0.07412493, 0.08451543,\n",
       "        0.        ],\n",
       "       [0.30304576, 0.        , 0.14285714, ..., 0.        , 0.11952286,\n",
       "        0.        ],\n",
       "       [0.10101525, 0.14285714, 0.        , ..., 0.        , 0.11952286,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.07412493, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08451543, 0.11952286, 0.11952286, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz_similaridades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSxslPyRIxNr"
   },
   "source": [
    "13.3) - Depois que construímos a matriz de similaridades, faça o ranking das sentenças utilizando o método ``pagerank`` do pacote `networkx`. Este método irá retornar um score para cada sentença. Para utilizar este método, os dados devem representados em um grafo. Para isto converta a matriz de similaridades para um grafo de similaridades, utilizando o método `network.from_numpy_array(matrix)`.\n",
    "\n",
    "<i>Mais informações Pagerank: </i>https://www.agenciamestre.com/marketing-digital/o-que-e-pagerank/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "qzcT5ognIxNs"
   },
   "outputs": [],
   "source": [
    "grafo = nx.from_numpy_array(matriz_similaridades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = nx.pagerank(grafo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 0.004021447721179649),\n",
       " (19, 0.004021447721179649),\n",
       " (15, 0.005655670158681687),\n",
       " (5, 0.007559632483027835),\n",
       " (28, 0.011320861876775489),\n",
       " (6, 0.01132219808546154),\n",
       " (36, 0.01245876816915345),\n",
       " (17, 0.012708394204105302),\n",
       " (24, 0.013217971142945685),\n",
       " (33, 0.013841617778291656),\n",
       " (38, 0.0146017509648837),\n",
       " (12, 0.015218666187634662),\n",
       " (14, 0.01613465308785664),\n",
       " (23, 0.016518064910325904),\n",
       " (30, 0.017089361220365713),\n",
       " (29, 0.017806214843770155),\n",
       " (31, 0.019035833922522026),\n",
       " (35, 0.02059076662993843),\n",
       " (3, 0.02094614009349831),\n",
       " (9, 0.024800621105511577),\n",
       " (27, 0.025031278859730773),\n",
       " (25, 0.026914108600851566),\n",
       " (37, 0.028719727389980774),\n",
       " (18, 0.030061246033376107),\n",
       " (2, 0.03034500646445102),\n",
       " (34, 0.030376390921891507),\n",
       " (20, 0.03668863971721915),\n",
       " (7, 0.036869708930105674),\n",
       " (11, 0.03733034100407843),\n",
       " (4, 0.038398763844942826),\n",
       " (26, 0.03872999251957474),\n",
       " (8, 0.0392727163458728),\n",
       " (13, 0.04030790516801762),\n",
       " (1, 0.0405190736215747),\n",
       " (10, 0.040656936621152974),\n",
       " (21, 0.04155965540620828),\n",
       " (32, 0.04826587262576661),\n",
       " (0, 0.05355270343067541),\n",
       " (22, 0.05752985018741965)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP2r-dMAIxNw"
   },
   "source": [
    "13.4) Ordene os scores retornados pelo `pagerank` e imprima as <b> 5 </b> sentenças com maior score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "p0vpIWCXIxNx"
   },
   "outputs": [],
   "source": [
    "index_selecionados = (sorted(scores.items(), key=operator.itemgetter(1), reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_rank = [i[0] for i in index_selecionados]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\n",
    "for i in indices_rank:\n",
    "    texto += \" \" + noticia_sents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No sábado (29), a Secretaria Estadual de Saúde e o Ministério da Saúde confirmaram o segundo caso de coronavírus no estado.  A Secretaria Estadual de Saúde de São Paulo divulgou nesta terça-feira (3) que caiu o número de pacientes com suspeita de coronavírus no estado. Após a confirmação de um caso, o governo estadual anunciou a criação de um centro de contingência para monitorar casos de coronavírus no estado. Segundo a metodologia da Secretaria Estadual de Saúde, para um caso ser considerado suspeito é necessário que o paciente tenha apresentado, além dos sintomas, histórico de viagem ou contato com caso suspeito. “Existem contactantes [do segundo caso], eles estão sendo monitorados, mas a gente não está divulgando mais números de contactantes por conta da privacidade das pessoas”, disse Paulo Menezes, coordenador do comitê de operações emergenciais (COE) do estado.'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAPFvduYIxN0"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "14) Analise o resultado da sumarização, construindo uma crítica sobre a qualidade e relevância da mesma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4u1ozylIxN1"
   },
   "source": [
    "O sumário ficou bem conciso e objetivo. O texto resumiu bem os fatos que ocorreram na época dos primeiros casos de COVID falando sobre os primeiros números de casos e os locais onde ocorreram. Achei interessante ter adicionado uma citação do coordenador do COE, Paulo Menezes. O texto parece ter sido escrito por uma humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqfzs38pIxN4"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "15) Faça o exercício 13 mais uma vez, agora substituindo a representação BoW pela TFIDF, usando o TDIDFVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "R28P0409IxN5"
   },
   "outputs": [],
   "source": [
    "def similaridade_TfIDF(s1, s2):\n",
    "    palavras_unicas = (set(s1 + s2))\n",
    "    model = TfidfVectorizer(binary=True)\n",
    "    model.fit(palavras_unicas)\n",
    "    vetor_s1 = model.transform([' '.join(s1)]).todense()\n",
    "    vetor_s2 = model.transform([' '.join(s2)]).todense()    \n",
    "    return cosine_similarity(vetor_s1, vetor_s2).reshape(-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "kiLNeKlkIxNm"
   },
   "outputs": [],
   "source": [
    "qtde_sents = len(noticia_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor_similaridades = np.zeros((len(noticia_sents) , len(noticia_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3030457633656632"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similaridade(noticias_processado[0], noticias_processado[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linha in range(0, qtde_sents):\n",
    "    for coluna in range(0, qtde_sents):\n",
    "        if linha == coluna:\n",
    "            continue\n",
    "        vetor_similaridades[linha][coluna] = similaridade_TfIDF(noticias_processado[linha], noticias_processado[coluna])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_similarides_tfidf(sents):\n",
    "    qtde_sents = len(noticia_sents)\n",
    "    vetor_similaridades = np.zeros((len(noticia_sents) , len(noticia_sents)))\n",
    "    for linha in range(0, qtde_sents):\n",
    "        for coluna in range(0, qtde_sents):\n",
    "            if linha == coluna:\n",
    "                continue\n",
    "            vetor_similaridades[linha][coluna] = similaridade_TfIDF(noticias_processado[linha], noticias_processado[coluna])\n",
    "    \n",
    "    return vetor_similaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_similaridades = matriz_similarides_tfidf(noticias_processado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.30304576, 0.10101525, ..., 0.07412493, 0.08451543,\n",
       "        0.        ],\n",
       "       [0.30304576, 0.        , 0.14285714, ..., 0.        , 0.11952286,\n",
       "        0.        ],\n",
       "       [0.10101525, 0.14285714, 0.        , ..., 0.        , 0.11952286,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.07412493, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08451543, 0.11952286, 0.11952286, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz_similaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "qzcT5ognIxNs"
   },
   "outputs": [],
   "source": [
    "grafo = nx.from_numpy_array(matriz_similaridades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = nx.pagerank(grafo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 0.004021447721179649),\n",
       " (19, 0.004021447721179649),\n",
       " (15, 0.005655670158681687),\n",
       " (5, 0.007559632483027836),\n",
       " (28, 0.01132086187677549),\n",
       " (6, 0.011322198085461537),\n",
       " (36, 0.01245876816915345),\n",
       " (17, 0.012708394204105302),\n",
       " (24, 0.013217971142945685),\n",
       " (33, 0.01384161777829166),\n",
       " (38, 0.014601750964883704),\n",
       " (12, 0.015218666187634664),\n",
       " (14, 0.016134653087856644),\n",
       " (23, 0.016518064910325897),\n",
       " (30, 0.017089361220365713),\n",
       " (29, 0.017806214843770162),\n",
       " (31, 0.019035833922522026),\n",
       " (35, 0.020590766629938435),\n",
       " (3, 0.02094614009349831),\n",
       " (9, 0.024800621105511584),\n",
       " (27, 0.025031278859730777),\n",
       " (25, 0.02691410860085157),\n",
       " (37, 0.028719727389980777),\n",
       " (18, 0.030061246033376114),\n",
       " (2, 0.03034500646445103),\n",
       " (34, 0.030376390921891518),\n",
       " (20, 0.036688639717219154),\n",
       " (7, 0.036869708930105674),\n",
       " (11, 0.03733034100407844),\n",
       " (4, 0.038398763844942826),\n",
       " (26, 0.03872999251957474),\n",
       " (8, 0.03927271634587281),\n",
       " (13, 0.04030790516801762),\n",
       " (1, 0.04051907362157469),\n",
       " (10, 0.04065693662115298),\n",
       " (21, 0.0415596554062083),\n",
       " (32, 0.04826587262576662),\n",
       " (0, 0.05355270343067542),\n",
       " (22, 0.057529850187419664)]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(scores.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "p0vpIWCXIxNx"
   },
   "outputs": [],
   "source": [
    "index_selecionados = (sorted(scores.items(), key=operator.itemgetter(1), reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_rank = [i[0] for i in index_selecionados]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_tfidf = \"\"\n",
    "for i in indices_rank:\n",
    "    texto_tfidf += \" \" + noticia_sents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No sábado (29), a Secretaria Estadual de Saúde e o Ministério da Saúde confirmaram o segundo caso de coronavírus no estado.  A Secretaria Estadual de Saúde de São Paulo divulgou nesta terça-feira (3) que caiu o número de pacientes com suspeita de coronavírus no estado. Após a confirmação de um caso, o governo estadual anunciou a criação de um centro de contingência para monitorar casos de coronavírus no estado. Segundo a metodologia da Secretaria Estadual de Saúde, para um caso ser considerado suspeito é necessário que o paciente tenha apresentado, além dos sintomas, histórico de viagem ou contato com caso suspeito. “Existem contactantes [do segundo caso], eles estão sendo monitorados, mas a gente não está divulgando mais números de contactantes por conta da privacidade das pessoas”, disse Paulo Menezes, coordenador do comitê de operações emergenciais (COE) do estado.'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No sábado (29), a Secretaria Estadual de Saúde e o Ministério da Saúde confirmaram o segundo caso de coronavírus no estado.  A Secretaria Estadual de Saúde de São Paulo divulgou nesta terça-feira (3) que caiu o número de pacientes com suspeita de coronavírus no estado. Após a confirmação de um caso, o governo estadual anunciou a criação de um centro de contingência para monitorar casos de coronavírus no estado. Segundo a metodologia da Secretaria Estadual de Saúde, para um caso ser considerado suspeito é necessário que o paciente tenha apresentado, além dos sintomas, histórico de viagem ou contato com caso suspeito. “Existem contactantes [do segundo caso], eles estão sendo monitorados, mas a gente não está divulgando mais números de contactantes por conta da privacidade das pessoas”, disse Paulo Menezes, coordenador do comitê de operações emergenciais (COE) do estado.'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW122tJlIxN8"
   },
   "source": [
    "<b> Atividade </b>\n",
    "\n",
    "16) Faça uma comparação dos resultados de sumarização obtidos pelas técnicas de Bag of Words e TFIDF, explicitando as diferenças observadas de forma concreta e exemplificando suas análises com frases dos textos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S48MAugpIxN9"
   },
   "source": [
    "O texto gerado automaticamente foi o mesmo para as duas abordagens. Possivelmente por conta do parâmetro \"binary=True\" informado em ambos os métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FM1YVoWKIxOB"
   },
   "source": [
    "<b> Desafio! (opcional, valendo pontos extras) </b>\n",
    "\n",
    "Implemente e analise os resultados de uma solução que faça a sumarização dos textos utilizando Word Embeddings + WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60kbPHNxIxOC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aula 5 - IAAM 31.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
